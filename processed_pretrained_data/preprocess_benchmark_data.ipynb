{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Linking Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"/harddisk/user/keminglu/evaluation_corpus/benchmarks\"\n",
    "output_dir = \"/harddisk/user/keminglu/evaluation_corpus/processed_benchmarks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(input_dir, \"genre\", \"aida-dev-kilt.jsonl\")) as f:\n",
    "    data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_train(sample):\n",
    "    target = f\"{{\\\"entities\\\": [{{\\\"mention\\\": \\\"{sample['meta']['mention']}\\\", \\\"title\\\": \\\"{sample['output'][0]['answer']}\\\"}}], \\\"triplets\\\": []}}\"\n",
    "    output = {\n",
    "        'rephrased_input': '\"' + sample['input'].replace(\"[START_ENT] \", \"\").replace(\" [END_ENT]\", \"\").strip() + '.\"' + \"\\n\\n\" + \"Please extract 1 entity from the context.\",\n",
    "        'target': target,\n",
    "        'id': sample['id'],\n",
    "    }\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(output_dir, \"aida-dev-kilt\"), \"w\") as f:\n",
    "    for sample in data:\n",
    "        f.write(json.dumps(transform_train(sample)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(record, data_file):\n",
    "    context = record['input'].replace(\"[START_ENT] \", \"\").replace(\" [END_ENT]\", \"\").strip()\n",
    "    prompt = '{\"entities\": [{\"mention\": \"%s\",' % record['meta']['mention']\n",
    "    return {\n",
    "        \"dataset\": data_file,\n",
    "        \"rephrased_inputs\": f\"\\\"{context}\\\"\\n\\nPlease identify 1 entity in the context.\" + \" \" + prompt,\n",
    "        \"prompt\": prompt,\n",
    "        \"true\": [each['answer'] for each in record['output']],\n",
    "        \"orig_id\": record[\"id\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = [\n",
    "    \"ace2004-test-kilt.jsonl\",\n",
    "    \"aida-test-kilt.jsonl\",\n",
    "    \"aquaint-test-kilt.jsonl\",\n",
    "    \"clueweb-test-kilt.jsonl\",\n",
    "    \"msnbc-test-kilt.jsonl\",\n",
    "]\n",
    "all_ie_data = []\n",
    "for data_file in data_files:\n",
    "    input_path = os.path.join(input_dir, \"entity_linking\", data_file)\n",
    "    for line in open(input_path).readlines():\n",
    "        processed_line = transform(json.loads(line), data_file) \n",
    "        all_ie_data.append(processed_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(output_dir, \"entity_linking\", \"all_ie_data\"), \"w\") as f:\n",
    "    for line in all_ie_data:\n",
    "        f.write(json.dumps(line) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_text': '\"30 panda cubs born in China in 2006 Current total of giant pandas bred in captivity now 217 BEIJING A mini baby boom last year has pushed up the number of pandas bred in captivity in China to 217 state media said Wednesday Some 34 pandas were born by artificial insemination in 2006 and 30 survived both record numbers for the endangered species Cao Qingyao a spokesman for the State Forestry Administration was quoted as saying by the Xinhua News Agency The previous record was the 21 baby pandas born in China s zoos and breeding centers in 2005 China has been raising pandas through artificial insemination for nearly 50 years mostly at two research facilities in the southwestern province of Sichuan In 2006 17 cubs were born at the Wolong Giant Panda Protection and Research Center and 12 at the Chengdu Research Base The other panda was bred at the zoo in the southwestern city of Chongqing The panda is one of the world s rarest animals with about 1 590 living in the wild in China mostly in Sichuan\"\\n\\nPlease identify 1 entity in the context. {\"entities\": [{\"mention\": \"Xinhua News Agency\",',\n",
       " 'prompt': '{\"entities\": [{\"mention\": \"Xinhua News Agency\",',\n",
       " 'true': ['Xinhua News Agency'],\n",
       " 'orig_id': 320}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(input_dir, \"entity_linking\", \"msnbc-test-kilt.jsonl\")) as f:\n",
    "    data = [json.loads(line) for line in f.readlines()]\n",
    "transform(data[320])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"/harddisk/user/keminglu/evaluation_corpus/benchmarks/open_entity/crowd/test.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(record):\n",
    "    return {\n",
    "        'input_text': \" \".join(record['left_context_token'] + [record['mention_span']] + record['right_context_token']) + \"\\n\\nPlease identify 1 entity in the sentence.\",\n",
    "        'prompt': '{\"entities\": [{\"mention\": \"%s\",' % record['mention_span'],\n",
    "        'true': record['y_str'],\n",
    "        'orig_id': record['annot_id']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(output_dir, \"entity_typing\", \"ufet_test.json\"), \"w\") as f:\n",
    "    for line in open(data_file).readlines():\n",
    "        processed_line = transform(json.loads(line))\n",
    "        f.write(json.dumps(processed_line) + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset conllpp (/harddisk/user/keminglu/evaluation_corpus/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "901eeb2bb0134cabb1d2dfa37ad56c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"conllpp\", cache_dir=\"/harddisk/user/keminglu/evaluation_corpus/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_dict = {\n",
    "    1: \"person\",\n",
    "    3: \"organization\",\n",
    "    5: \"location\",\n",
    "    7: \"miscellaneous\"\n",
    "}\n",
    "\n",
    "def transform(record, hint=\"none\"):\n",
    "    tokens = record['tokens']\n",
    "    input_text = \" \".join(tokens)\n",
    "    mentions = []\n",
    "    flag = True\n",
    "    for token, tag in zip(tokens, record['ner_tags']):\n",
    "        if tag != 0:\n",
    "            if flag:\n",
    "                mentions.append([(token, tag)])\n",
    "                flag = False\n",
    "            else:\n",
    "                mentions[-1].append((token, tag))\n",
    "        else:\n",
    "            flag = True\n",
    "    mentions = [(\" \".join([item[0] for item in each]), type_dict[each[0][1]]) for each in mentions]\n",
    "    if hint == \"none\":\n",
    "        prompt = 'Please identify all entities in the types Person, Organization, Location, Miscellaneous.'\n",
    "    elif hint == \"gold\":\n",
    "        prompt = 'Please identify %d entities in the types Person, Organization, Location, Miscellaneous.' % len(mentions)\n",
    "    elif hint == \"large\":\n",
    "        prompt = 'Please identify %d entities in the types Person, Organization, Location, Miscellaneous.' % 5\n",
    "    return {\n",
    "        \"dataset\": \"conllpp\",\n",
    "        \"split\": hint,\n",
    "        \"rephrased_inputs\": f\"\\\"{input_text}\\\"\\n\\n{prompt}\",\n",
    "        \"true\": mentions,\n",
    "        \"orig_id\": record[\"id\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'conllpp',\n",
       " 'split': 'none',\n",
       " 'rephrased_inputs': '\"Hosts UAE play Kuwait and South Korea take on Indonesia on Saturday in Group A matches .\"\\n\\nPlease identify all entities in the types Person, Organization, Location, Miscellaneous.',\n",
       " 'true': [('UAE', 'location'),\n",
       "  ('Kuwait', 'location'),\n",
       "  ('South Korea', 'location'),\n",
       "  ('Indonesia', 'location')],\n",
       " 'orig_id': '20'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform(dataset['test'][20], \"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ner_data = []\n",
    "for hint in (\"none\", \"gold\"):\n",
    "    for sample in dataset[\"test\"]:\n",
    "        all_ner_data.append(transform(sample, hint)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'conllpp',\n",
       " 'split': 'gold',\n",
       " 'rephrased_inputs': '\"BUFFALO AT SEATTLE\"\\n\\nPlease identify 2 entities in the types Person, Organization, Location, Miscellaneous.',\n",
       " 'true': [('BUFFALO', 'organization'), ('SEATTLE', 'location')],\n",
       " 'orig_id': '547'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ner_data[4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(output_dir, \"ner\", \"conllpp_large.jsonl\"), \"w\") as f:\n",
    "    for sample in dataset[\"test\"]:\n",
    "        f.write(json.dumps(transform(sample, \"large\")) + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrossNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_mapping = {\n",
    "        'programlang': 'Programming language',\n",
    "        'country': 'Country',\n",
    "        'field': 'Field',\n",
    "        'algorithm': 'Algorithm',\n",
    "        'organisation': 'Organization',\n",
    "        'task': 'Task',\n",
    "        'university': 'University',\n",
    "        'product': 'Product',\n",
    "        'misc': 'Miscellaneous',\n",
    "        'conference': 'Conference',\n",
    "        'location': 'Location',\n",
    "        'researcher': 'Researcher',\n",
    "        'metrics': 'Metrics',\n",
    "        'person': 'Person',\n",
    "        'magazine': 'Magazine',\n",
    "        'award': 'Award',\n",
    "        'event': 'Event',\n",
    "        'poem': 'Poem',\n",
    "        'book': 'Book',\n",
    "        'literarygenre': 'Literary genre',\n",
    "        'writer': 'Writer',\n",
    "        'song': 'Song',\n",
    "        'musicalinstrument': 'Muscial instrument',\n",
    "        'album': 'Album',\n",
    "        'musicalartist': 'Musical artist',\n",
    "        'band': 'Band',\n",
    "        'musicgenre': 'Music genre',\n",
    "        'politician': 'Politician',\n",
    "        'politicalparty': 'Political party',\n",
    "        'election': 'Election',\n",
    "        'astronomicalobject': 'Astronomical object',\n",
    "        'enzyme': 'Enzyme',\n",
    "        'scientist': 'Scientist',\n",
    "        'chemicalelement': 'Chemical element',\n",
    "        'academicjournal': 'Academic journal',\n",
    "        'university': 'University',\n",
    "        'theory': 'Theory',\n",
    "        'protein': 'Protein',\n",
    "        'chemicalcompound': 'Chemical compound',\n",
    "        'discipline': 'Discipline',\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def transform(split, hint=\"none\"):\n",
    "    with open(os.path.join(\"/harddisk/user/keminglu/evaluation_corpus/benchmarks/CrossNER/ner_data\", split, \"test.txt\")) as f:\n",
    "        data = []\n",
    "        tmp = []\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            if line == '\\n':\n",
    "                data.append(copy.deepcopy(tmp))\n",
    "                tmp = []\n",
    "            else:\n",
    "                tmp.append(line.strip().split(\"\\t\"))\n",
    "            line = f.readline()\n",
    "    \n",
    "    processed_data = []\n",
    "    for idx, sample in enumerate(data):\n",
    "        mentions = []\n",
    "        flag = True\n",
    "        for token, tag in sample:\n",
    "            if tag != 'O':\n",
    "                if flag:\n",
    "                    mentions.append([(token, tag)])\n",
    "                    flag = False\n",
    "                else:\n",
    "                    mentions[-1].append((token, tag))\n",
    "            else:\n",
    "                flag = True\n",
    "        mentions = [(\" \".join([item[0] for item in each]), each[0][1].split(\"-\")[1]) for each in mentions]\n",
    "        tokens = [each[0] for each in sample]\n",
    "        string = \" \".join(tokens)\n",
    "        processed_data.append((string, mentions))\n",
    "    \n",
    "    types = set([item[1] for each in processed_data for item in each[1]])\n",
    "    print(types)\n",
    "    types = \", \".join([type_mapping[t] for t in types])\n",
    "\n",
    "    final_processed_data = []\n",
    "    for idx, (string, mentions) in enumerate(processed_data):\n",
    "        if hint == \"none\":\n",
    "            prompt = f\"Please identify all entities in the types {types}.\"\n",
    "        elif hint == \"gold\":\n",
    "            prompt = f\"Please identify %d entities in the types {types}.\" % len(mentions)\n",
    "        elif hint == \"large\":\n",
    "            prompt = f\"Please identify %d entities in the types {types}.\" % 5\n",
    "        final_processed_data.append( {\n",
    "            \"dataset\": f\"crossner_{split}\",\n",
    "            \"split\": hint,\n",
    "            \"rephrased_inputs\": f\"\\\"{string}\\\"\\n\\n{prompt}\",\n",
    "            \"true\": mentions,\n",
    "            \"orig_id\": idx\n",
    "        })\n",
    "    return final_processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chemicalcompound', 'award', 'protein', 'organisation', 'discipline', 'theory', 'scientist', 'location', 'person', 'academicjournal', 'chemicalelement', 'university', 'enzyme', 'country', 'event', 'misc', 'astronomicalobject'}\n"
     ]
    }
   ],
   "source": [
    "tmp = transform(\"science\", \"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'product', 'conference', 'country', 'metrics', 'organisation', 'programlang', 'task', 'location', 'person', 'field', 'university', 'researcher', 'algorithm', 'misc'}\n",
      "{'product', 'conference', 'country', 'metrics', 'organisation', 'programlang', 'task', 'location', 'person', 'field', 'university', 'researcher', 'algorithm', 'misc'}\n",
      "{'literarygenre', 'award', 'poem', 'magazine', 'organisation', 'book', 'location', 'person', 'country', 'event', 'misc', 'writer'}\n",
      "{'literarygenre', 'award', 'poem', 'magazine', 'organisation', 'book', 'location', 'person', 'country', 'event', 'misc', 'writer'}\n",
      "{'song', 'award', 'musicalartist', 'organisation', 'band', 'musicgenre', 'person', 'location', 'country', 'album', 'event', 'misc', 'musicalinstrument'}\n",
      "{'song', 'award', 'musicalartist', 'organisation', 'band', 'musicgenre', 'person', 'location', 'country', 'album', 'event', 'misc', 'musicalinstrument'}\n",
      "{'election', 'organisation', 'politicalparty', 'politician', 'location', 'person', 'country', 'event', 'misc'}\n",
      "{'election', 'organisation', 'politicalparty', 'politician', 'location', 'person', 'country', 'event', 'misc'}\n",
      "{'chemicalcompound', 'award', 'protein', 'organisation', 'discipline', 'theory', 'scientist', 'location', 'person', 'academicjournal', 'chemicalelement', 'university', 'enzyme', 'country', 'event', 'misc', 'astronomicalobject'}\n",
      "{'chemicalcompound', 'award', 'protein', 'organisation', 'discipline', 'theory', 'scientist', 'location', 'person', 'academicjournal', 'chemicalelement', 'university', 'enzyme', 'country', 'event', 'misc', 'astronomicalobject'}\n"
     ]
    }
   ],
   "source": [
    "for split in (\"ai\", \"literature\", \"music\", \"politics\", \"science\"):\n",
    "    for hint in (\"none\", \"gold\"):\n",
    "        processed_data = transform(split, hint)\n",
    "        all_ner_data.extend(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {('conllpp', 'none'): 3453,\n",
       "             ('conllpp', 'gold'): 3453,\n",
       "             ('crossner_ai', 'none'): 431,\n",
       "             ('crossner_ai', 'gold'): 431,\n",
       "             ('crossner_literature', 'none'): 416,\n",
       "             ('crossner_literature', 'gold'): 416,\n",
       "             ('crossner_music', 'none'): 465,\n",
       "             ('crossner_music', 'gold'): 465,\n",
       "             ('crossner_politics', 'none'): 651,\n",
       "             ('crossner_politics', 'gold'): 651,\n",
       "             ('crossner_science', 'none'): 543,\n",
       "             ('crossner_science', 'gold'): 543})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "cnt = defaultdict(lambda: 0)\n",
    "for each in all_ner_data:\n",
    "    cnt[(each['dataset'], each['split'])] += 1\n",
    "cnt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/harddisk/user/keminglu/evaluation_corpus/processed_benchmarks/ner/all_ner_data\", \"w\") as f:\n",
    "    for sample in all_ner_data:\n",
    "        f.write(json.dumps(sample) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in (\"ai\", \"literature\", \"music\", \"politics\", \"science\"):\n",
    "    for hint in (\"none\", \"gold\", \"large\"):\n",
    "        processed_data = transform(split, hint)\n",
    "        with open(os.path.join(\"/harddisk/user/keminglu/evaluation_corpus/processed_benchmarks/ner\", f\"crossner_{split}_{hint}.jsonl\"), \"w\") as f:\n",
    "            for sample in processed_data:\n",
    "                f.write(json.dumps(sample) + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"/harddisk/user/keminglu/evaluation_corpus/benchmarks/Re-TACRED/data/test.json\"\n",
    "data = json.load(open(data_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_briskets(input_str):\n",
    "    input_str = input_str.replace(\"-LRB-\", \"(\").replace(\"-RRB-\", \")\")\n",
    "    input_str = input_str.replace(\"-LSB-\", \"[\").replace(\"-RSB-\", \"]\")\n",
    "    return input_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(output_dir, \"relation_extraction\", \"retacred_test.jsonl\"), \"w\") as f:\n",
    "    for each in data:\n",
    "        token = each['token']\n",
    "        sent = replace_briskets(\" \".join(token))\n",
    "        subj = replace_briskets(\" \".join(token[each['subj_start']:each['subj_end']+1]))\n",
    "        obj = replace_briskets(\" \".join(token[each['obj_start']:each['obj_end']+1]))\n",
    "        subj_type = each['subj_type']\n",
    "        obj_type = each['obj_type']\n",
    "        ent_string = ['{\"mention\": \"%s\", \"title\": \"%s\", \"type\": [\"%s\"]}' % (ent[0], ent[0], ent[1].lower()) for ent in [(subj, subj_type), (obj, obj_type)]]\n",
    "        ent_string = \", \".join(ent_string)\n",
    "        record = {\n",
    "            \"input_text\": sent,\n",
    "            #\"prompt\": '{\"number of entities\": 2, \"entities\": [%s], \"triplets\": [{\"head\": \"%s\", \"tail\": \"%s\", \"relations\":' % (ent_string, subj, obj),\n",
    "            \"prompt\": '{\"number of entities\": 2, \"entities\": [%s], \"triplets\":' % (ent_string),\n",
    "            \"true\": each['relation']\n",
    "        }\n",
    "        f.write(json.dumps(record) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"/harddisk/user/keminglu/evaluation_corpus/benchmarks/redocred/test_revised.json\"\n",
    "data = json.load(open(data_file))\n",
    "rel_map = json.load(open(\"/harddisk/user/keminglu/evaluation_corpus/benchmarks/redocred/rel_info.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE_DICT = {\n",
    "    \"PER\": \"person\",\n",
    "    \"MISC\": \"other\",\n",
    "    \"LOC\": \"location\",\n",
    "    \"NUM\": \"number\",\n",
    "    \"TIME\": \"time\",\n",
    "    \"ORG\": \"organization\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(output_dir, \"relation_extraction\", \"redocred_test.jsonl\"), \"w\") as f:\n",
    "    for each in data:\n",
    "        sent = \" \".join(map(\" \".join, each['sents']))\n",
    "        ent_string = ['{\"mention\": \"%s\", \"title\": \"%s\", \"type\": [\"%s\"]}' % (ent['name'], ent['name'], TYPE_DICT[ent['type']]) for ent in sum(each['vertexSet'], [])]\n",
    "        ent_string = list(set(ent_string))\n",
    "        num_of_ent = len(ent_string)\n",
    "        ent_string = \", \".join(ent_string)\n",
    "        true = [(each['vertexSet'][triplet['h']][0]['name'], each['vertexSet'][triplet['t']][0]['name'], rel_map[triplet['r']]) for triplet in each['labels']]\n",
    "        record = {\n",
    "                \"input_text\": sent,\n",
    "                \"prompt\": '{\"number of entities\": %d, \"entities\": [%s], \"triplets\":' % (num_of_ent, ent_string),\n",
    "                \"true\": true\n",
    "            }\n",
    "        f.write(json.dumps(record) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-process",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4c793426adf016a6deb589c78ef9b52b0d2671097f5c49eb7a518bfd664e7c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
