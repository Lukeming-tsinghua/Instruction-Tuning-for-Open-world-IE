{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import copy\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import accelerate\n",
    "from evaluate_utils import *\n",
    "from tqdm.notebook import tqdm\n",
    "from evaluate import load"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Dispatch loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda:7\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39m/harddisk/user/keminglu/bigscience_tokenizer\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39m/data/home/keminglu/workspace/devcloud/finetune_7b_data_v1_epoch_1\u001b[39;49m\u001b[39m\"\u001b[39;49m, dtype\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mfp16\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/local_workspace/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:464\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    463\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 464\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    465\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    466\u001b[0m     )\n\u001b[1;32m    467\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    468\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    469\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    470\u001b[0m )\n",
      "File \u001b[0;32m~/local_workspace/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/modeling_utils.py:2362\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2359\u001b[0m     init_contexts\u001b[39m.\u001b[39mappend(init_empty_weights())\n\u001b[1;32m   2361\u001b[0m \u001b[39mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m-> 2362\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(config, \u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m   2364\u001b[0m \u001b[39m# Check first if we are `from_pt`\u001b[39;00m\n\u001b[1;32m   2365\u001b[0m \u001b[39mif\u001b[39;00m use_keep_in_fp32_modules:\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'dtype'"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:7\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/harddisk/user/keminglu/bigscience_tokenizer\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/data/home/keminglu/workspace/devcloud/finetune_7b_data_v1_epoch_1\", torch_dtype=torch.float16).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset conll2003 (/harddisk/user/keminglu/evaluation_corpus/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b43b686603c14ed0bbfd0047b82752ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"conll2003\", cache_dir=\"/harddisk/user/keminglu/evaluation_corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /harddisk/user/keminglu/evaluation_corpus/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98/cache-d11d703ce4b96364.arrow\n"
     ]
    }
   ],
   "source": [
    "ner_idx_map = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n",
    "ner_idx_map_inv = {item:key for key, item in ner_idx_map.items()}\n",
    "ner_type_map = {\"B-PER\": \"Human\", \"B-ORG\": \"Organization\", \"B-LOC\": \"Location\", \"B-MISC\": \"MISC\"}\n",
    "\n",
    "def process(record):\n",
    "    tokens = record['tokens']\n",
    "    ner_tags = record['ner_tags']\n",
    "    \n",
    "    ner = []\n",
    "    tmp = None\n",
    "    for token, tag in zip(tokens, ner_tags):\n",
    "        if tag != 0:\n",
    "            if not tmp:\n",
    "                tmp = [[token], [ner_idx_map_inv[tag]]]\n",
    "            else:\n",
    "                tmp[0].append(token)\n",
    "                tmp[1].append(ner_idx_map_inv[tag])\n",
    "        else:\n",
    "            if tmp:\n",
    "                x = copy.deepcopy(tmp)\n",
    "                x[0] = \" \".join(x[0])\n",
    "                x[1] = ner_type_map[x[1][0]]\n",
    "                ner.append(x)\n",
    "                tmp = None\n",
    "    text = \" \".join(tokens)\n",
    "    return {\"text\": text, \"label\": ner}\n",
    "        \n",
    "data = dataset['test'].map(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A woman was charged on Friday with terrorist offences after three Irish Republican Army mortar bombs were found in a Belfast house , police said .',\n",
       " [['Irish Republican Army', 'Organization'], ['Belfast', 'Location']])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 2000\n",
    "data[index]['text'], data[index]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('BELFAST', ['Astronomical survey'])]\n"
     ]
    }
   ],
   "source": [
    "result = infer(data[1000]['text'], \"\", model, tokenizer, device)[0]\n",
    "\n",
    "entities = json.loads(result)['entities']\n",
    "outputs = [(item[\"mention\"], item[\"type\"]) for item in entities]\n",
    "print(outputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-to-text Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c59fdd9a3aa4cd58704a66b486f2132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:7\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/harddisk/user/keminglu/bigscience_tokenizer\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/data/home/keminglu/workspace/devcloud/finetune_7b_data_v1_epoch_1\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset web_nlg (/harddisk/user/keminglu/evaluation_corpus/web_nlg/release_v2/0.0.0/28ffb892f7f42450dd9558684aa43bcaf44b1b3bf0d77cb8d73534646af88dda)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a6fb57fe17a49c3a05fe054643f6903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"web_nlg\", \"release_v2\", cache_dir=\"/harddisk/user/keminglu/evaluation_corpus/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def transform(text):\n",
    "    text = text.strip()\n",
    "    if \"_\" in text:\n",
    "        return text.replace(\"_\", \" \")\n",
    "    else:\n",
    "        return re.sub('(?<=[a-z])[A-Z]|(?<!^)[A-Z](?=[a-z])', ' \\g<0>', text).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59b587662fec47f59145038d067d79e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "records = []\n",
    "references = []\n",
    "for each in tqdm(dataset['test']):\n",
    "    record = {\"entities\": [], \"triplets\": []}\n",
    "    ref = each['lex']['text']\n",
    "    each = each['modified_triple_sets']['mtriple_set']\n",
    "    entities = set()\n",
    "    relations = defaultdict(list)\n",
    "    for group in each:\n",
    "        for triplet in group:\n",
    "            h, r, t = triplet.split(\"|\")\n",
    "            h = transform(h)\n",
    "            r = transform(r)\n",
    "            t = transform(t)\n",
    "            entities.add(h)\n",
    "            entities.add(t)\n",
    "            relations[(h, t)].append(r)\n",
    "    for ent in entities:\n",
    "        record[\"entities\"].append({\"mention\": ent, \"title\": ent})\n",
    "    for h, t in relations:\n",
    "        record[\"triplets\"].append({\"head\": h, \"tail\": t, \"relations\": [{\"title\": r} for r in relations[(h, t)]]})\n",
    "    records.append(record)\n",
    "    references.append(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entities': [{'mention': 'Rhythm and blues', 'title': 'Rhythm and blues'},\n",
       "  {'mention': 'Andra (singer)', 'title': 'Andra (singer)'}],\n",
       " 'triplets': [{'head': 'Andra (singer)',\n",
       "   'tail': 'Rhythm and blues',\n",
       "   'relations': [{'title': 'genre'}]}]}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records[60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = json.dumps(records[60])\n",
    "\n",
    "def generate(context):\n",
    "    inputs = tokenizer(context, return_tensors=\"pt\")['input_ids'].to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs=inputs,\n",
    "            num_beams=4,\n",
    "            do_sample=False,\n",
    "            length_penalty=1,\n",
    "            max_new_tokens=256)\n",
    "    \n",
    "    prompt_length = len(\n",
    "        tokenizer.decode(\n",
    "            inputs[0],\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True\n",
    "        )\n",
    "    )\n",
    "    texts = tokenizer.batch_decode(\n",
    "        outputs, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    text = [text[prompt_length:].strip() for text in texts]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9af7230f58d46eea3dfb46a342dc04a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outputs = []\n",
    "for record in tqdm(records):\n",
    "    context = json.dumps(record)\n",
    "    output = generate(context)\n",
    "    outputs.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.1035299838676711, 'precisions': [0.2648573692551506, 0.13361513687600643, 0.075439852700491, 0.0430324459234609], 'brevity_penalty': 1.0, 'length_ratio': 3.042613465131698, 'translation_length': 100960, 'reference_length': 33182}\n"
     ]
    }
   ],
   "source": [
    "predictions = [output[0] for output in outputs]\n",
    "bleu = load(\"bleu\")\n",
    "results = bleu.compute(predictions=predictions, references=references[:len(predictions)])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluated_outputs = []\n",
    "for inp, pred, ref in zip(records, outputs, references):\n",
    "    report = {\"input\": inp, \"output\": pred, \"annotation\": ref, \"source\": \"webnlg\"}\n",
    "    evaluated_outputs.append(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "mongodb_config = {\"host\": '10.12.192.31', \"port\": 27017}\n",
    "client = MongoClient(**mongodb_config)\n",
    "evaluation_collection = client['structure_lm']['evaluation_inverse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x7fc0c217b430>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_collection.insert_many(evaluated_outputs, ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wiki evaluation\n",
    "with open(\"/harddisk/user/keminglu/evaluation_corpus/wiki_eval_output.txt\") as f:\n",
    "    inputs = [json.loads(line) for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(ann):\n",
    "    record = {\"entities\": [], \"triplets\": []}\n",
    "    for ent in sum(ann['ents'], []):\n",
    "        item = {\"mention\": ent[\"text\"], \"title\": ent[\"title\"]}\n",
    "        if \"description\" in ent:\n",
    "            item[\"description\"] = ent[\"description\"]\n",
    "        if \"aliases\" in ent and len(ent[\"aliases\"]) > 0:\n",
    "            item[\"aliases\"] = ent[\"aliases\"]\n",
    "        if \"type\" in ent and len(ent[\"type\"]) > 0:\n",
    "            item[\"type\"] = ent[\"type\"]\n",
    "        record[\"entities\"].append(item)\n",
    "    for rel in ann[\"relations\"]:\n",
    "        item = {\n",
    "            \"head\": rel[\"subject\"][\"name\"],\n",
    "            \"tail\": rel[\"object\"][\"name\"],\n",
    "            \"relations\": [{\n",
    "                \"title\": each[\"name\"],\n",
    "                \"description\": {\"english\": each[\"description\"][\"english\"]},\n",
    "                \"aliases\": {\"english\": each[\"aliases\"][\"english\"]}\n",
    "                } for each in rel[\"property\"]]}\n",
    "        record[\"triplets\"].append(item)\n",
    "        \n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for each in inputs:\n",
    "    records.append(transform(each[\"annotates\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e79296480474d1eb7cc26b3984a894d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outputs = []\n",
    "for record in tqdm(records):\n",
    "    context = json.dumps(record)\n",
    "    output = generate(context)\n",
    "    outputs.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluated_outputs = []\n",
    "for inp, pred, annotate in zip(records, outputs, inputs):\n",
    "    annotate = annotate[\"input\"]\n",
    "    report = {\"input\": inp, \"output\": pred, \"annotate\": annotate, \"source\": \"wiki\"}\n",
    "    evaluated_outputs.append(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "mongodb_config = {\"host\": '10.12.192.31', \"port\": 27017}\n",
    "client = MongoClient(**mongodb_config)\n",
    "evaluation_collection = client['structure_lm']['evaluation_inverse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x7fc0c21d9520>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_collection.insert_many(evaluated_outputs, ordered=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "278bb6c2e99a4c13a918e1d089331786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:7\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/harddisk/user/keminglu/bigscience_tokenizer\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/data/home/keminglu/workspace/devcloud/finetune_7b_data_v1_epoch_1\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 7; 31.75 GiB total capacity; 30.14 GiB already allocated; 11.75 MiB free; 30.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m context \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m百度3月16号即将发布中国第一个大模型文言一心。\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m infer(context, \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m, model, tokenizer, device, length_penalty\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n",
      "File \u001b[0;32m/harddisk/user/keminglu/evaluate/evaluate_utils.py:31\u001b[0m, in \u001b[0;36minfer\u001b[0;34m(context, prompt, model, tokenizer, device, num_beams, do_sample, length_penalty, max_new_tokens, skip_special_tokens)\u001b[0m\n\u001b[1;32m     28\u001b[0m max_new_tokens \u001b[39m=\u001b[39m max_new_tokens \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(inputs[\u001b[39m0\u001b[39m])\n\u001b[1;32m     30\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 31\u001b[0m     outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m     32\u001b[0m         inputs\u001b[39m=\u001b[39;49minputs,\n\u001b[1;32m     33\u001b[0m         num_beams\u001b[39m=\u001b[39;49mnum_beams,\n\u001b[1;32m     34\u001b[0m         do_sample\u001b[39m=\u001b[39;49mdo_sample,\n\u001b[1;32m     35\u001b[0m         length_penalty\u001b[39m=\u001b[39;49mlength_penalty,\n\u001b[1;32m     36\u001b[0m         max_new_tokens\u001b[39m=\u001b[39;49mmax_new_tokens)\n\u001b[1;32m     38\u001b[0m prompt_length \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\n\u001b[1;32m     39\u001b[0m     tokenizer\u001b[39m.\u001b[39mdecode(\n\u001b[1;32m     40\u001b[0m         inputs[\u001b[39m0\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     )\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     45\u001b[0m texts \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_decode(\n\u001b[1;32m     46\u001b[0m     outputs, skip_special_tokens\u001b[39m=\u001b[39mskip_special_tokens)\n",
      "File \u001b[0;32m~/local_workspace/anaconda3/envs/transformers/lib/python3.8/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/local_workspace/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/generation/utils.py:1474\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1468\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1469\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[1;32m   1470\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1471\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m     \u001b[39m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1474\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeam_search(\n\u001b[1;32m   1475\u001b[0m         input_ids,\n\u001b[1;32m   1476\u001b[0m         beam_scorer,\n\u001b[1;32m   1477\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1478\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1479\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1480\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1481\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1482\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1483\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1484\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1485\u001b[0m     )\n\u001b[1;32m   1487\u001b[0m \u001b[39melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[1;32m   1488\u001b[0m     \u001b[39m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1489\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[0;32m~/local_workspace/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/generation/utils.py:2722\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2718\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   2720\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m-> 2722\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2723\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2724\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2725\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2726\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2727\u001b[0m )\n\u001b[1;32m   2729\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2730\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/local_workspace/anaconda3/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/local_workspace/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/models/bloom/modeling_bloom.py:900\u001b[0m, in \u001b[0;36mBloomForCausalLM.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **deprecated_arguments)\u001b[0m\n\u001b[1;32m    896\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot unexpected arguments: \u001b[39m\u001b[39m{\u001b[39;00mdeprecated_arguments\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    898\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 900\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m    901\u001b[0m     input_ids,\n\u001b[1;32m    902\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    903\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    904\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    905\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    906\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    907\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    908\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    909\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    910\u001b[0m )\n\u001b[1;32m    911\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    913\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/local_workspace/anaconda3/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/local_workspace/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/models/bloom/modeling_bloom.py:782\u001b[0m, in \u001b[0;36mBloomModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, **deprecated_arguments)\u001b[0m\n\u001b[1;32m    774\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    775\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    776\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    779\u001b[0m         head_mask[i],\n\u001b[1;32m    780\u001b[0m     )\n\u001b[1;32m    781\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 782\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    783\u001b[0m         hidden_states,\n\u001b[1;32m    784\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    785\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mcausal_mask,\n\u001b[1;32m    786\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    787\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    788\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    789\u001b[0m         alibi\u001b[39m=\u001b[39;49malibi,\n\u001b[1;32m    790\u001b[0m     )\n\u001b[1;32m    792\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    793\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/local_workspace/anaconda3/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/local_workspace/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/models/bloom/modeling_bloom.py:463\u001b[0m, in \u001b[0;36mBloomBlock.forward\u001b[0;34m(self, hidden_states, alibi, attention_mask, layer_past, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    460\u001b[0m     residual \u001b[39m=\u001b[39m attention_output\n\u001b[1;32m    462\u001b[0m \u001b[39m# MLP.\u001b[39;00m\n\u001b[0;32m--> 463\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(layernorm_output, residual)\n\u001b[1;32m    465\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n\u001b[1;32m    466\u001b[0m     outputs \u001b[39m=\u001b[39m (output,) \u001b[39m+\u001b[39m outputs\n",
      "File \u001b[0;32m~/local_workspace/anaconda3/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/local_workspace/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/models/bloom/modeling_bloom.py:384\u001b[0m, in \u001b[0;36mBloomMLP.forward\u001b[0;34m(self, hidden_states, residual)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor, residual: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 384\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgelu_impl(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense_h_to_4h(hidden_states))\n\u001b[1;32m    386\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpretraining_tp \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslow_but_exact:\n\u001b[1;32m    387\u001b[0m         intermediate_output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros_like(residual)\n",
      "File \u001b[0;32m~/local_workspace/anaconda3/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/local_workspace/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/models/bloom/modeling_bloom.py:208\u001b[0m, in \u001b[0;36mBloomGelu.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[39mreturn\u001b[39;00m GeLUFunction\u001b[39m.\u001b[39mapply(x)\n\u001b[1;32m    207\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 208\u001b[0m     \u001b[39mreturn\u001b[39;00m bloom_gelu_forward(x)\n",
      "File \u001b[0;32m~/local_workspace/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/models/bloom/modeling_bloom.py:158\u001b[0m, in \u001b[0;36mbloom_gelu_forward\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbloom_gelu_forward\u001b[39m(x: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m    150\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[39m    Custom bias GELU function. Adapted from Megatron-DeepSpeed code. Here we use a simple implementation (inference) to\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[39m    make the model jitable.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39m            input hidden states\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m     \u001b[39mreturn\u001b[39;00m x \u001b[39m*\u001b[39m \u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mtanh(\u001b[39m0.79788456\u001b[39m \u001b[39m*\u001b[39m x \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39m0.044715\u001b[39;49m \u001b[39m*\u001b[39;49m x \u001b[39m*\u001b[39;49m x)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 7; 31.75 GiB total capacity; 30.14 GiB already allocated; 11.75 MiB free; 30.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "context = \"百度3月16号即将发布中国第一个大模型文言一心。\"\n",
    "infer(context, \"\", model, tokenizer, device, length_penalty=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_file_path = \"/harddisk/user/keminglu/evaluation_corpus/wiki_eval.txt\"\n",
    "ann_file_path = \"/harddisk/user/keminglu/evaluation_corpus/wiki_eval.ann\"\n",
    "data = open(corpus_file_path).readlines()\n",
    "annotates = open(ann_file_path).readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a4802d927334b93a994ee2b1fcec08f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 7; 31.75 GiB total capacity; 30.16 GiB already allocated; 11.75 MiB free; 30.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m pbar \u001b[39m=\u001b[39m tqdm(total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(data))\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m each, ann \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(data, annotates):\n\u001b[0;32m----> 5\u001b[0m     res \u001b[39m=\u001b[39m infer(each, \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m, model, tokenizer, device, num_beams\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m      6\u001b[0m     all_res\u001b[39m.\u001b[39mappend({\u001b[39m\"\u001b[39m\u001b[39minput\u001b[39m\u001b[39m\"\u001b[39m: each, \u001b[39m\"\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m: res, \u001b[39m\"\u001b[39m\u001b[39mannotates\u001b[39m\u001b[39m\"\u001b[39m: json\u001b[39m.\u001b[39mloads(ann)})\n\u001b[1;32m      7\u001b[0m     pbar\u001b[39m.\u001b[39mupdate(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/harddisk/user/keminglu/evaluate/evaluate_utils.py:31\u001b[0m, in \u001b[0;36minfer\u001b[0;34m(context, prompt, model, tokenizer, device, num_beams, do_sample, length_penalty, max_new_tokens, skip_special_tokens)\u001b[0m\n\u001b[1;32m     28\u001b[0m max_new_tokens \u001b[39m=\u001b[39m max_new_tokens \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(inputs[\u001b[39m0\u001b[39m])\n\u001b[1;32m     30\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 31\u001b[0m     outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m     32\u001b[0m         inputs\u001b[39m=\u001b[39;49minputs,\n\u001b[1;32m     33\u001b[0m         num_beams\u001b[39m=\u001b[39;49mnum_beams,\n\u001b[1;32m     34\u001b[0m         do_sample\u001b[39m=\u001b[39;49mdo_sample,\n\u001b[1;32m     35\u001b[0m         length_penalty\u001b[39m=\u001b[39;49mlength_penalty,\n\u001b[1;32m     36\u001b[0m         max_new_tokens\u001b[39m=\u001b[39;49mmax_new_tokens)\n\u001b[1;32m     38\u001b[0m prompt_length \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\n\u001b[1;32m     39\u001b[0m     tokenizer\u001b[39m.\u001b[39mdecode(\n\u001b[1;32m     40\u001b[0m         inputs[\u001b[39m0\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     )\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     45\u001b[0m texts \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_decode(\n\u001b[1;32m     46\u001b[0m     outputs, skip_special_tokens\u001b[39m=\u001b[39mskip_special_tokens)\n",
      "File \u001b[0;32m~/local_workspace/anaconda3/envs/transformers/lib/python3.8/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/local_workspace/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/generation/utils.py:1391\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[1;32m   1385\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1386\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnum_return_sequences has to be 1, but is \u001b[39m\u001b[39m{\u001b[39;00mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences\u001b[39m}\u001b[39;00m\u001b[39m when doing\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1387\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m greedy search.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1388\u001b[0m         )\n\u001b[1;32m   1390\u001b[0m     \u001b[39m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1391\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgreedy_search(\n\u001b[1;32m   1392\u001b[0m         input_ids,\n\u001b[1;32m   1393\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1394\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1395\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1396\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1397\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1398\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1399\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1400\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1401\u001b[0m     )\n\u001b[1;32m   1403\u001b[0m \u001b[39melif\u001b[39;00m is_contrastive_search_gen_mode:\n\u001b[1;32m   1404\u001b[0m     \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mnum_return_sequences \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/local_workspace/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/generation/utils.py:2179\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2176\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2178\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2179\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2180\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2181\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2182\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2183\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2184\u001b[0m )\n\u001b[1;32m   2186\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2187\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/local_workspace/anaconda3/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/local_workspace/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/models/bloom/modeling_bloom.py:900\u001b[0m, in \u001b[0;36mBloomForCausalLM.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **deprecated_arguments)\u001b[0m\n\u001b[1;32m    896\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot unexpected arguments: \u001b[39m\u001b[39m{\u001b[39;00mdeprecated_arguments\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    898\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 900\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m    901\u001b[0m     input_ids,\n\u001b[1;32m    902\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    903\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    904\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    905\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    906\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    907\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    908\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    909\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    910\u001b[0m )\n\u001b[1;32m    911\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    913\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/local_workspace/anaconda3/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/local_workspace/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/models/bloom/modeling_bloom.py:782\u001b[0m, in \u001b[0;36mBloomModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, **deprecated_arguments)\u001b[0m\n\u001b[1;32m    774\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    775\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    776\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    779\u001b[0m         head_mask[i],\n\u001b[1;32m    780\u001b[0m     )\n\u001b[1;32m    781\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 782\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    783\u001b[0m         hidden_states,\n\u001b[1;32m    784\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    785\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mcausal_mask,\n\u001b[1;32m    786\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    787\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    788\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    789\u001b[0m         alibi\u001b[39m=\u001b[39;49malibi,\n\u001b[1;32m    790\u001b[0m     )\n\u001b[1;32m    792\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    793\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/local_workspace/anaconda3/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/local_workspace/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/models/bloom/modeling_bloom.py:463\u001b[0m, in \u001b[0;36mBloomBlock.forward\u001b[0;34m(self, hidden_states, alibi, attention_mask, layer_past, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    460\u001b[0m     residual \u001b[39m=\u001b[39m attention_output\n\u001b[1;32m    462\u001b[0m \u001b[39m# MLP.\u001b[39;00m\n\u001b[0;32m--> 463\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(layernorm_output, residual)\n\u001b[1;32m    465\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n\u001b[1;32m    466\u001b[0m     outputs \u001b[39m=\u001b[39m (output,) \u001b[39m+\u001b[39m outputs\n",
      "File \u001b[0;32m~/local_workspace/anaconda3/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/local_workspace/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/models/bloom/modeling_bloom.py:384\u001b[0m, in \u001b[0;36mBloomMLP.forward\u001b[0;34m(self, hidden_states, residual)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor, residual: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 384\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgelu_impl(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense_h_to_4h(hidden_states))\n\u001b[1;32m    386\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpretraining_tp \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslow_but_exact:\n\u001b[1;32m    387\u001b[0m         intermediate_output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros_like(residual)\n",
      "File \u001b[0;32m~/local_workspace/anaconda3/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/local_workspace/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/models/bloom/modeling_bloom.py:208\u001b[0m, in \u001b[0;36mBloomGelu.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[39mreturn\u001b[39;00m GeLUFunction\u001b[39m.\u001b[39mapply(x)\n\u001b[1;32m    207\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 208\u001b[0m     \u001b[39mreturn\u001b[39;00m bloom_gelu_forward(x)\n",
      "File \u001b[0;32m~/local_workspace/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/models/bloom/modeling_bloom.py:158\u001b[0m, in \u001b[0;36mbloom_gelu_forward\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbloom_gelu_forward\u001b[39m(x: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m    150\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[39m    Custom bias GELU function. Adapted from Megatron-DeepSpeed code. Here we use a simple implementation (inference) to\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[39m    make the model jitable.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39m            input hidden states\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m     \u001b[39mreturn\u001b[39;00m x \u001b[39m*\u001b[39;49m \u001b[39m0.5\u001b[39;49m \u001b[39m*\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mtanh(\u001b[39m0.79788456\u001b[39m \u001b[39m*\u001b[39m x \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39m0.044715\u001b[39m \u001b[39m*\u001b[39m x \u001b[39m*\u001b[39m x)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 7; 31.75 GiB total capacity; 30.16 GiB already allocated; 11.75 MiB free; 30.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "all_res = []\n",
    "data = data[:100]\n",
    "pbar = tqdm(total=len(data))\n",
    "for each, ann in zip(data, annotates):\n",
    "    res = infer(each, \"\", model, tokenizer, device, num_beams=1)\n",
    "    all_res.append({\"input\": each, \"output\": res, \"annotates\": json.loads(ann)})\n",
    "    pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(corpus_file_path.replace(\".txt\", \"_output.txt\"), \"w\") as f:\n",
    "    for res in all_res:\n",
    "        f.write(json.dumps(res) + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_mapping_file_path = \"/harddisk/data/nlp_data/kb/wikidata/20210520/mapping/property_names.json\"\n",
    "entity_mapping_file_path = \"/harddisk/data/nlp_data/kb/wikidata/20210520/mapping/qid2sitelinks.enwiki.title.json\"\n",
    "mongodb_config = {\"host\": '10.12.192.31', \"port\": 27017}\n",
    "extractor = KGExtractor(mongodb_config, entity_mapping_file_path, property_mapping_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '29900074', 'title': 'Alfa Romeo Tipo 312', 'ents': [[{'end': 162, 'id': '10386732', 'start': 143, 'text': 'Alfa Romeo Tipo 308', 'title': 'Alfa Romeo Tipo 308', 'qid': 'Q785879', 'description': {'english': 'car'}, 'aliases': {}, 'type': []}, {'end': 209, 'id': '29901461', 'start': 190, 'text': 'Alfa Romeo Tipo 316', 'title': 'Alfa Romeo Tipo 316', 'qid': 'Q3611023', 'description': {}, 'aliases': {}, 'type': []}, {'end': 227, 'id': '199965', 'start': 217, 'text': 'V16 engine', 'title': 'V16 engine', 'qid': 'Q2263764', 'description': {'english': 'piston engine with 16 cylinders in vee configuration'}, 'aliases': {'english': ['V16 engine', 'V-16']}, 'type': ['Engine configuration']}], [{'end': 51, 'id': '10578247', 'start': 34, 'text': 'Alfa Romeo 12C-37', 'title': 'Alfa Romeo 12C', 'qid': 'Q2306794', 'description': {'english': 'motor vehicle'}, 'aliases': {}, 'type': ['Car model']}], []], 'n_ents': 4, 'n_mapped_ent': 4, 'relations': [], 'n_rel_pair': 0, 'n_rel': 0}\n"
     ]
    }
   ],
   "source": [
    "with open(\"/harddisk/user/keminglu/evaluation_corpus/wiki_eval_output.txt\") as f:\n",
    "    outputs = [json.loads(line) for line in f.readlines()]\n",
    "print(outputs[0]['annotates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/harddisk/user/keminglu/evaluation_corpus/wiki_eval_output_with_evaluation.txt\", \"w\") as f:\n",
    "    for output in outputs:\n",
    "        report = evaluator(output)\n",
    "        output.update({\"evaluation\": report})\n",
    "        f.write(json.dumps(output) + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push evaluation results into DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongodb_config = {\"host\": '10.12.192.31', \"port\": 27017}\n",
    "client = MongoClient(**mongodb_config)\n",
    "evaluation_collection = client['structure_lm']['evaluation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/harddisk/user/keminglu/evaluation_corpus/wiki_eval_output_with_evaluation.txt\") as f:\n",
    "    data = [json.loads(line) for line in f.readlines()]\n",
    "for i in range(len(data)):\n",
    "    data[i][\"output\"][0] = json.loads(data[i][\"output\"][0])\n",
    "    data[i].update({\"source\": \"wiki\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x7f3b45ee9fa0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_collection.insert_many(data, ordered=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fceb5c14fd5bb9497f9330c2eeb829ea1da1239a1871fb05da80190d45a37701"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
